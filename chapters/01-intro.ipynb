{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5uxZ8O5xYzC"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/langchain-course/blob/main/chapters/01-intro.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mptJD94oH6ak"
      },
      "source": [
        "#### LangChain Essentials Course"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwjxctC0H6am"
      },
      "source": [
        "# Getting Started with LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adOQXfBHH6am"
      },
      "source": [
        "LangChain is one of the most popular open source libraries for AI Engineers. It's goal is to abstract away the complexity in building AI software, provide easy-to-use building blocks, and make it easier when switching between AI service providers.\n",
        "\n",
        "In this example, we will introduce LangChain, building a simple LLM-powered assistant. We'll provide examples for both OpenAI's `gpt-4o-mini` *and* Meta's `llama3.2` via Ollama!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccz9ouBwH6am"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ We will be using OpenAI for this example allowing us to run everything via API. If you would like to use Ollama instead, check out the [Ollama LangChain Course](https://github.com/aurelio-labs/langchain-course/tree/main/notebooks/ollama).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXoe53VyIAD-"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "  langchain-core  \\\n",
        "  langchain-openai \\\n",
        "  langchain-community \\\n",
        "  langchain\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFy86kIBH6am"
      },
      "source": [
        "## Initializing OpenAI's gpt-4o-mini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jq_58vDH6an"
      },
      "source": [
        "We start by initializing our LLM. We will use OpenAI's `gpt-4o-mini` model, if you need an API key you can get one from [OpenAI's website](https://platform.openai.com/settings/organization/api-keys)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dL1OqS7RH6an"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Define local variables for endpoint and key\n",
        "azure_openai_key = userdata.get('Azure_OpenAI_Key')\n",
        "azure_openai_endpoint = userdata.get('Azure_OpenAI_Endpoint')\n",
        "\n",
        "# Optionally set environment variables as well (LangChain can use these too)\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = azure_openai_key\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = azure_openai_endpoint\n",
        "\n",
        "# You might also need to set the API version depending on your deployment\n",
        "# os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"YYYY-MM-DD\" # Replace with your API version\n",
        "\n",
        "openai_model = \"gpt-4o\" # This will now refer to your deployment name in Azure"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.environ[\"AZURE_OPENAI_ENDPOINT\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBvvJuV33YEY",
        "outputId": "9285c850-bb40-4809-dc7e-aaf25432b551"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://genairecon.openai.azure.com/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Y_tz8kTVH6an",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e4ea3f7-ba69-44ec-e1b2-7625809ccd20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3473: UserWarning: WARNING! azure_endpoint is not default parameter.\n",
            "                azure_endpoint was transferred to model_kwargs.\n",
            "                Please confirm that azure_endpoint is what you intended.\n",
            "  if (await self.run_code(code, result,  async_=asy)):\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3473: UserWarning: WARNING! api_version is not default parameter.\n",
            "                api_version was transferred to model_kwargs.\n",
            "                Please confirm that api_version is what you intended.\n",
            "  if (await self.run_code(code, result,  async_=asy)):\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3473: UserWarning: WARNING! azure_endpoint is not default parameter.\n",
            "                azure_endpoint was transferred to model_kwargs.\n",
            "                Please confirm that azure_endpoint is what you intended.\n",
            "  if (await self.run_code(code, result,  async_=asy)):\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3473: UserWarning: WARNING! api_version is not default parameter.\n",
            "                api_version was transferred to model_kwargs.\n",
            "                Please confirm that api_version is what you intended.\n",
            "  if (await self.run_code(code, result,  async_=asy)):\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.caches import InMemoryCache, BaseCache\n",
        "\n",
        "# Assuming azure_openai_endpoint and azure_openai_key are defined in a previous cell\n",
        "# If not, you'll need to define them here or run the cell where they are defined\n",
        "\n",
        "# For normal accurate responses\n",
        "llm = ChatOpenAI(\n",
        "    temperature=0.0,\n",
        "    model=openai_model, # This is your deployment name in Azure\n",
        "    azure_endpoint=azure_openai_endpoint, # Pass the endpoint explicitly\n",
        "    api_key=azure_openai_key, # Pass the API key explicitly\n",
        "    api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\") # Keep using env var for api version or pass explicitly if needed\n",
        ")\n",
        "\n",
        "# For unique creative responses\n",
        "creative_llm = ChatOpenAI(\n",
        "    temperature=0.9,\n",
        "    model=openai_model, # This is your deployment name in Azure\n",
        "    azure_endpoint=azure_openai_endpoint, # Pass the endpoint explicitly\n",
        "    api_key=azure_openai_key, # Pass the API key explicitly\n",
        "    api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\") # Keep using env var for api version or pass explicitly if needed\n",
        ")\n",
        "\n",
        "ChatOpenAI.model_rebuild()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jymjwQiMH6an"
      },
      "source": [
        "We will be taking an `article` _draft_ and using LangChain to generate various useful items around this article. We'll be creating:\n",
        "\n",
        "1. An article title\n",
        "2. An article description\n",
        "3. Editor advice where we will insert an additional paragraph in the article\n",
        "4. A thumbnail / hero image for our article."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSsQ8xKuH6an"
      },
      "source": [
        "Here we input our article to start with. Currently this is using an article from the Aurelio AI learning page."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfwEVT97H6ao"
      },
      "outputs": [],
      "source": [
        "article = \"\"\"\n",
        "\\\n",
        "We believe AI's short—to mid-term future belongs to agents and that the long-term future of *AGI* may evolve from agentic systems. Our definition of agents covers any neuro-symbolic system in which we merge neural AI (such as an LLM) with semi-traditional software.\n",
        "\n",
        "With agents, we allow LLMs to integrate with code — allowing AI to search the web, perform math, and essentially integrate into anything we can build with code. It should be clear the scope of use cases is phenomenal where AI can integrate with the broader world of software.\n",
        "\n",
        "In this introduction to AI agents, we will cover the essential concepts that make them what they are and why that will make them the core of real-world AI in the years to come.\n",
        "\n",
        "---\n",
        "\n",
        "## Neuro-Symbolic Systems\n",
        "\n",
        "Neuro-symbolic systems consist of both neural and symbolic computation, where:\n",
        "\n",
        "- Neural refers to LLMs, embedding models, or other neural network-based models.\n",
        "- Symbolic refers to logic containing symbolic logic, such as code.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM0HBv7bH6ao"
      },
      "source": [
        "## Preparing our Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmFcbFU4H6ao"
      },
      "source": [
        "LangChain comes with several prompt classes and methods for organizing or constructing our prompts. We will cover these in more detail in later examples, but for now we'll cover the essentials that we need here.\n",
        "\n",
        "Prompts for chat agents are at a minimum broken up into three components, those are:\n",
        "\n",
        "* System prompt: this provides the instructions to our LLM on how it must behave, what it's objective is, etc.\n",
        "\n",
        "* User prompt: this is a user written input.\n",
        "\n",
        "* AI prompt: this is the AI generated output. When representing a conversation, previous generations will be inserted back into the next prompt and become part of the broader _chat history_.\n",
        "\n",
        "```\n",
        "You are a helpful AI assistant, you will do XYZ.    | SYSTEM PROMPT\n",
        "\n",
        "User: Hi, what is the capital of Australia?         | USER PROMPT\n",
        "AI: It is Canberra                                  | AI PROMPT\n",
        "User: When is the best time to visit?               | USER PROMPT\n",
        "```\n",
        "\n",
        "LangChain provides us with _templates_ for each of these prompt types. By using templates we can insert different inputs to the template, modifying the prompt based on the provided inputs.\n",
        "\n",
        "Let's initialize our system and user prompt first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gsWvNLcH6ao"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "# Defining the system prompt (how the AI should act)\n",
        "system_prompt = SystemMessagePromptTemplate.from_template(\n",
        "    \"You are an AI assistant that helps generate article titles.\"\n",
        ")\n",
        "\n",
        "# the user prompt is provided by the user, in this case however the only dynamic\n",
        "# input is the article\n",
        "user_prompt = HumanMessagePromptTemplate.from_template(\n",
        "    \"\"\"You are tasked with creating a name for a article.\n",
        "The article is here for you to examine {article}\n",
        "\n",
        "The name should be based of the context of the article.\n",
        "Be creative, but make sure the names are clear, catchy,\n",
        "and relevant to the theme of the article.\n",
        "\n",
        "Only output the article name, no other explanation or\n",
        "text can be provided.\"\"\",\n",
        "    input_variables=[\"article\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaCnQaAdH6ao"
      },
      "source": [
        "We can display what our formatted human prompt would look like after inserting a value into the `article` parameter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3PlnjtEH6ao"
      },
      "outputs": [],
      "source": [
        "user_prompt.format(article=\"TEST STRING\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDbNHkm3H6ap"
      },
      "source": [
        "We have our system and user prompts, we can merge both into our full chat prompt using the `ChatPromptTemplate`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeokbXtIH6ap"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "first_prompt = ChatPromptTemplate.from_messages([system_prompt, user_prompt])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K70F86jH6ap"
      },
      "source": [
        "By default, the `ChatPromptTemplate` will read the `input_variables` from each of the prompt templates inserted and allow us to use those input variables when formatting the full chat prompt template:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHnbZ5iUH6ap"
      },
      "outputs": [],
      "source": [
        "print(first_prompt.format(article=\"TEST STRING\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-5Sv9XoH6ap"
      },
      "source": [
        "`ChatPromptTemplate` also prefixes each individual message with it's role, ie `System:`, `Human:`, or `AI:`.\n",
        "\n",
        "We can chain together our `first_prompt` template and the `llm` object we defined earlier to create a simple LLM chain. This chain will perform the steps **prompt formatting > llm generation > get output**.\n",
        "\n",
        "We'll be using **L**ang**C**hain **E**xpression **L**anguage (LCEL) to construct our chain. This syntax can look a little strange but we will cover it in detail later in the course. For now, all we need to know is that we define our inputs with the first dictionary segment (ie `{\"article\": lambda x: x[\"article\"]}`) and then we use the pipe operator (`|`) to say that the output from the left of the pipe will be fed into the input to the right of the pipe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7z1PNdmxSCJY"
      },
      "outputs": [],
      "source": [
        "chain_one = (\n",
        "    {\"article\": lambda x: x[\"article\"]}\n",
        "    | first_prompt\n",
        "    | creative_llm\n",
        "    | {\"article_title\": lambda x: x.content}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcXrW5aoH6ap"
      },
      "source": [
        "Our first chain creates the article title, note: we can run all of these individually..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKLWN56xH6ap"
      },
      "outputs": [],
      "source": [
        "article_title_msg = chain_one.invoke({\"article\": article})\n",
        "article_title_msg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6iDE_RwH6ap"
      },
      "source": [
        "But we will actually chain this step with multiple other `LLMChain` steps. So, to continue, our next step is to summarize the article using both the `article` and newly generated `article_title` values, from which we will output a new `summary` variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtLcHVxSH6ap"
      },
      "outputs": [],
      "source": [
        "second_user_prompt = HumanMessagePromptTemplate.from_template(\n",
        "    \"\"\"You are tasked with creating a description for\n",
        "the article. The article is here for you to examine:\n",
        "\n",
        "---\n",
        "\n",
        "{article}\n",
        "\n",
        "---\n",
        "\n",
        "Here is the article title '{article_title}'.\n",
        "\n",
        "Output the SEO friendly article description. Do not output\n",
        "anything other than the description.\"\"\",\n",
        "    input_variables=[\"article\", \"article_title\"]\n",
        ")\n",
        "\n",
        "second_prompt = ChatPromptTemplate.from_messages([\n",
        "    system_prompt,\n",
        "    second_user_prompt\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPFKMq8tcmw-"
      },
      "outputs": [],
      "source": [
        "chain_two = (\n",
        "    {\n",
        "        \"article\": lambda x: x[\"article\"],\n",
        "        \"article_title\": lambda x: x[\"article_title\"]\n",
        "    }\n",
        "    | second_prompt\n",
        "    | llm\n",
        "    | {\"summary\": lambda x: x.content}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdhUjj89efRw"
      },
      "outputs": [],
      "source": [
        "article_description_msg = chain_two.invoke({\n",
        "    \"article\": article,\n",
        "    \"article_title\": article_title_msg[\"article_title\"]\n",
        "})\n",
        "article_description_msg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SEGMX9BH6ap"
      },
      "source": [
        "The third step will consume our first `article` variable and provide several output fields, focusing on helping the user improve a part of their writing. As we are outputting multiple fields we can specify for the LLM to use structured outputs, keeping the generated fields aligned with our requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPQf5ycHH6ap"
      },
      "outputs": [],
      "source": [
        "third_user_prompt = HumanMessagePromptTemplate.from_template(\n",
        "    \"\"\"You are tasked with creating a new paragraph for the\n",
        "article. The article is here for you to examine:\n",
        "\n",
        "---\n",
        "\n",
        "{article}\n",
        "\n",
        "---\n",
        "\n",
        "Choose one paragraph to review and edit. During your edit\n",
        "ensure you provide constructive feedback to the user so they\n",
        "can learn where to improve their own writing.\"\"\",\n",
        "    input_variables=[\"article\"]\n",
        ")\n",
        "\n",
        "# prompt template 3: creating a new paragraph for the article\n",
        "third_prompt = ChatPromptTemplate.from_messages([\n",
        "    system_prompt,\n",
        "    third_user_prompt\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKUX9TFuDP0J"
      },
      "source": [
        "We create a pydantic object describing the output format we need. This format description is then passed to our model using the `with_structured_output` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1VHC0VSDdDM"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "\n",
        "class Paragraph(BaseModel):\n",
        "    original_paragraph: str = Field(description=\"The original paragraph\")\n",
        "    edited_paragraph: str = Field(description=\"The improved edited paragraph\")\n",
        "    feedback: str = Field(description=(\n",
        "        \"Constructive feedback on the original paragraph\"\n",
        "    ))\n",
        "\n",
        "structured_llm = creative_llm.with_structured_output(Paragraph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kOGsMRhEIyZ"
      },
      "source": [
        "Now we put all of this together in another chain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sIuyP-L-u7A"
      },
      "outputs": [],
      "source": [
        "# chain 3: inputs: article / output: article_para\n",
        "chain_three = (\n",
        "    {\"article\": lambda x: x[\"article\"]}\n",
        "    | third_prompt\n",
        "    | structured_llm\n",
        "    | {\n",
        "        \"original_paragraph\": lambda x: x.original_paragraph,\n",
        "        \"edited_paragraph\": lambda x: x.edited_paragraph,\n",
        "        \"feedback\": lambda x: x.feedback\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLnzeFPSEZkJ"
      },
      "outputs": [],
      "source": [
        "out = chain_three.invoke({\"article\": article})\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ-sfqNnH6aq"
      },
      "source": [
        "Now we want this article to look appealing, so we need to grab an image based of our article! However the prompt for the article image `cannot be over 1000 letters` so this has to be short in case we want to add anything in such as `style` later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7ew1WT4H6aq"
      },
      "outputs": [],
      "source": [
        "from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "image_prompt = PromptTemplate(\n",
        "    input_variables=[\"article\"],\n",
        "    template=(\n",
        "        \"Generate a prompt with less then 500 characters to generate an image \"\n",
        "        \"based on the following article: {article}\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjZ_rqPZH6aq"
      },
      "source": [
        "The `generate_and_display` function will generate the article image once we have the prompt from our image prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLxLXKS4H6aq"
      },
      "outputs": [],
      "source": [
        "from skimage import io\n",
        "import matplotlib.pyplot as plt\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "def generate_and_display_image(image_prompt):\n",
        "    image_url = DallEAPIWrapper().run(image_prompt)\n",
        "    image_data = io.imread(image_url)\n",
        "\n",
        "    # And update the display code to:\n",
        "    plt.imshow(image_data)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# we wrap this in a RunnableLambda for use with LCEL\n",
        "image_gen_runnable = RunnableLambda(generate_and_display_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3bIZVzcvkJJ"
      },
      "source": [
        "We have all of our image generation components ready, we chain them together again with LCEL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1QQtXtfAwRU"
      },
      "outputs": [],
      "source": [
        "# chain 4: inputs: article, article_para / outputs: new_suggestion_article\n",
        "chain_four = (\n",
        "    {\"article\": lambda x: x[\"article\"]}\n",
        "    | image_prompt\n",
        "    | llm\n",
        "    | (lambda x: x.content)\n",
        "    | image_gen_runnable\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHl5sfMNvr-J"
      },
      "source": [
        "And now, we `invoke` our final chain:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAU3jI0VvGlp"
      },
      "outputs": [],
      "source": [
        "chain_four.invoke({\"article\": article})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAA3_UuSH6av"
      },
      "source": [
        "With that we've built LLM chains that can help us building and writing articles. We've understood a few of the basics of LangChain, introduced **L**ang**C**hain **E**xpression **L**anguage (LCEL), and built a multi-modal article-helper pipeline."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}